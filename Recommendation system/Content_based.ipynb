{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "An_Content_based.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.8"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lMDhsq5qvgsf"
      },
      "source": [
        "Import libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a8APL1GlvwpY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "46e73fa3-5f49-47dc-aaf8-83c26a2ab498"
      },
      "source": [
        "!pip install pyspark"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pyspark\n",
            "  Downloading pyspark-3.2.0.tar.gz (281.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 281.3 MB 37 kB/s \n",
            "\u001b[?25hCollecting py4j==0.10.9.2\n",
            "  Downloading py4j-0.10.9.2-py2.py3-none-any.whl (198 kB)\n",
            "\u001b[K     |████████████████████████████████| 198 kB 53.2 MB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: pyspark\n",
            "  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyspark: filename=pyspark-3.2.0-py2.py3-none-any.whl size=281805912 sha256=7077b686a15d895a49c499da30b08617bdb3a85e50cca4a913aae5aad3b01582\n",
            "  Stored in directory: /root/.cache/pip/wheels/0b/de/d2/9be5d59d7331c6c2a7c1b6d1a4f463ce107332b1ecd4e80718\n",
            "Successfully built pyspark\n",
            "Installing collected packages: py4j, pyspark\n",
            "Successfully installed py4j-0.10.9.2 pyspark-3.2.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tDt8vXXut18D"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import time\n",
        "import pyspark\n",
        "from pyspark import SparkConf, SparkContext\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "import math\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H26z8dO4Dx4Q",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "af030f7c-36f1-4299-a33a-a56a990b599f"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive',force_remount=True)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1XEL7vhHwEML"
      },
      "source": [
        "Read file"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jZVh4ZxhwJ2P"
      },
      "source": [
        "df = pd.read_csv('movies.csv',converters={i: str for i in range(7668)})"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iaKrSBMx8ygl"
      },
      "source": [
        "Add Movie_id columns"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OH1bWWipA82W"
      },
      "source": [
        "df['Movie_id'] = [i for i in range(df.shape[0])]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sR52o23QHGMm"
      },
      "source": [
        "Transform all important feature into a string"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pyKWiYegHF7J"
      },
      "source": [
        "def transform_important_features(data):\n",
        "  important_features = []\n",
        "  for i in range(data.shape[0]):\n",
        "    important_features.append(data['genre'][i] + ' ' + data['year'][i] + ' ' + data['director'][i] + ' ' + data['writer'][i] + ' ' + data['star'][i])\n",
        "  return important_features"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H_SOe3sO8h-Y"
      },
      "source": [
        "Function to calculate Eclidean distance"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FqXIs1d_DoYG"
      },
      "source": [
        "def euclidean_dis(a,b):\n",
        "  res = 0\n",
        "  for i in range(len(a)):\n",
        "    res += (a[i] - b[i])**2\n",
        "  return math.sqrt(res)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sRz4EjJPwqwo"
      },
      "source": [
        "def Recommendation_System_Content_Based_Parallelized(data,movie_name,expectation):\n",
        "\n",
        "#create SparkContext\n",
        "  conf = SparkConf().setMaster(\"local\").setAppName(\"content-based-recommendation-system\").set(\"spark.executor.memory\", \"15g\")\n",
        "  sc = SparkContext.getOrCreate(conf=conf)\n",
        "\n",
        "#Reduce dimension and keep important features\n",
        "  selected_columns = ['genre','year','director','writer','star']\n",
        "  selected_df = data[selected_columns]\n",
        "\n",
        "#create columns containing important feartures\n",
        "  selected_df['important_features'] = transform_important_features(data)\n",
        "\n",
        "#get movie id by movie_name\n",
        "  movie_id = data[data.name == movie_name]['Movie_id'].values[0 ]\n",
        "\n",
        "#standardize the columns containing important features string by TfidVectorizer\n",
        "  ndarray_standardize_vectors = TfidfVectorizer().fit_transform(selected_df['important_features']).toarray()\n",
        "  #print(ndarray_standardize_vectors[:10])\n",
        "#using RDD to parallelize standardized vectors\n",
        "  vector_rdd = sc.parallelize(ndarray_standardize_vectors)\n",
        "\n",
        "#calculate distance from the vector of movie_name was typed to the rest \n",
        "  distance = vector_rdd.map(lambda x : euclidean_dis(x,ndarray_standardize_vectors[movie_id])).zipWithIndex().filter(lambda x : x[0] > 0.0).sortByKey(ascending=True)\n",
        "\n",
        "#take n = expectation shorest distance\n",
        "  movie_ids = distance.values().take(expectation)\n",
        "\n",
        "#append into a new list\n",
        "  recommended_movies = []\n",
        "  for id in movie_ids:\n",
        "    recommended_movies.append(data[data.Movie_id == id]['name'].values[0])\n",
        "\n",
        "#return recommended movie list\n",
        "  return recommended_movies"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5RdL-UcW1Crj"
      },
      "source": [
        "testing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C3Sf-MjA7Bne",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d904dda0-e48d-45c0-e5ce-dcd9b27dad17"
      },
      "source": [
        "start = time.time()\n",
        "print(Recommendation_System_Content_Based_Parallelized(df,\"Serial\",10))\n",
        "print(\"Execute time: \", time.time() - start)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Comfort and Joy', 'A Thin Line Between Love and Hate', 'My Bodyguard', 'Three Amigos!', 'L.A. Story', 'The Long Riders', 'Being Human', 'Loving Couples', 'Local Hero', 'Hero at Large']\n",
            "Execute time:  20.89415669441223\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mGPXWvBhr9ed",
        "outputId": "3c916d16-ac31-4f08-8b66-6e4fa5f0c278"
      },
      "source": [
        "start = time.time()\n",
        "print(Recommendation_System_Content_Based_Parallelized(df,\"Venom\",10))\n",
        "print(\"Execute time: \", time.time() - start)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['The Fiendish Plot of Dr. Fu Manchu', 'A Summer Story', 'The Four Seasons', \"King Solomon's Mines\", 'Allan Quatermain and the Lost City of Gold', 'Alive', 'Cat People', 'Exposed', 'DuckTales the Movie: Treasure of the Lost Lamp', 'Come See the Paradise']\n",
            "Execute time:  12.797738313674927\n"
          ]
        }
      ]
    }
  ]
}